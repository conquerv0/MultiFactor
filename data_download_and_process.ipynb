{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dde57df",
   "metadata": {},
   "source": [
    "#### Set up\n",
    "Run this section before anything else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5b8e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import Dataloader_ricequant as dl\n",
    "import pandas as pd\n",
    "import rqdatac as rq\n",
    "from constants import *\n",
    "import scipy\n",
    "import statsmodels as sm\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pathos\n",
    "from tqdm.notebook import tqdm\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27d7a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyParallel(dfGrouped, func):\n",
    "    #parrallel computing version of pd.groupby.apply, works most of the time but not always\n",
    "    #I mainly use it for cases where func takes in a dataframe and outputs a dataframe or a series\n",
    "    with pathos.multiprocessing.ProcessPool(pathos.helpers.cpu_count()) as pool:\n",
    "        ret_list = pool.map(func, [group for name, group in dfGrouped])\n",
    "    return pd.concat(ret_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692d4db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.rq_initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794fe9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_index_and_col(df) -> pd.DataFrame:\n",
    "    #sort the dataframe by index and column\n",
    "    return df.sort_index(axis=0).reindex(sorted(df.columns), axis=1)\n",
    "    #the following might achieve the same result in a cleaner way\n",
    "    # return df.sort_index(axis=0).sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecab5ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dl.load_basic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077c6ad8",
   "metadata": {},
   "source": [
    "#### Load Industry Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25fef49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indus_mapping = dl.load_industry_mapping()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165e0f95",
   "metadata": {},
   "source": [
    "#### Load Index Data\n",
    "In hiearachical backtesting we need weights of index(e.g. CSI300) data to make the portfolio to stay industry-neutral with the index.\n",
    "\n",
    "Currently index data is assumed to be uniformly weighted among all stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6434d4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSI 300 沪深300\n",
    "df_index = pd.read_csv(dl.DATAPATH + 'index_data/' + 'sh000300.csv',usecols=['date', 'open','close','change'],index_col=['date']).sort_index(ascending=True)\n",
    "df_index.columns = ['CSI_300_' + col for col in df_index.columns]\n",
    "df_index.index = df_index.index.values.astype('datetime64')\n",
    "df_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c722ed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "((df_index['CSI_300_change'] + 1).cumprod() - 1).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee36ca0",
   "metadata": {},
   "source": [
    "#### Download factor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107cd01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Data/raw_data/stock_names.h5', 'rb') as fp:\n",
    "     stock_names = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d9988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# value factor\n",
    "# value = ['pe_ratio_ttm','pcf_ratio_ttm', 'pcf_ratio_total_ttm','pb_ratio_ttm','book_to_market_ratio_ttm','dividend_yield_ttm', 'ps_ratio_ttm']\n",
    "# dl.download_factor_data(stock_names, value, START_DATE,END_DATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3a4fe7",
   "metadata": {},
   "source": [
    "### Data Preprocessing Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aefb63",
   "metadata": {},
   "source": [
    "Major Steps: \n",
    "\n",
    "0) Read all csv's and concatenate the desired column from each dataframe\n",
    "\n",
    "1) Filter out data before START_DATE and after END_DATE(backtesting period) from the raw stock data. \n",
    "\n",
    "- 剔除不在回测区间内的信息\n",
    "\n",
    "2) Filter listed stocks\n",
    "\n",
    "- 选出回测区间内每只股票上市的时间。这一步是为了步骤3，因为在每个选股日筛选ST或者停牌股的前提是股票在该选股日已上市。\n",
    "\n",
    "3) Filter out ST stocks, suspended stocks and stocks that are listed only within one year\n",
    "- 剔除ST，停牌和次新股（上市未满一年的股票）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c79a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 0\n",
    "df_backtest = pd.concat(results, axis=0).rename(columns={'code': 'stock'}).loc[:, INDEX_COLS + BASIC_INFO_COLS]\n",
    "df_backtest['date'] = pd.to_datetime(df_backtest['date'])\n",
    "\n",
    "# step 1\n",
    "df_backtest = df_backtest[ (START_DATE <= df_backtest['date']) & (df_backtest['date'] <= END_DATE) ]\n",
    "df_backtest['stock'] = df_backtest['stock'].apply(lambda stock: dl.normalize_code(stock))\n",
    "\n",
    "# have a (date_stock) multi-index dataframe \n",
    "df_backtest = df_backtest.set_index(INDEX_COLS).sort_index()\n",
    "df_backtest = df_backtest.unstack(level=1).stack(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe756f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check stock index\n",
    "stock_names = df_backtest.index.get_level_values(1).unique()\n",
    "stock_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe20b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Timedelta('1y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3b51d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backtest.index.get_level_values(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ea93f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backtest['listed_date'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bfe482",
   "metadata": {},
   "outputs": [],
   "source": [
    "listed_dates.astype('datetime64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e70fa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2\n",
    "# get the listed date\n",
    "listed_dates = {dl.normalize_code(result['code'][0]): result['date'].min() for result in results}\n",
    "listed_dates = pd.DataFrame(pd.Series(listed_dates), columns=['listed_date']).sort_index().astype('datetime64')\n",
    "# left join with dataframe 'listed_dates'\n",
    "df_backtest = df_backtest.merge(listed_dates, left_on = 'stock', right_index=True, how='left')\n",
    "# create a new variable called 'is_listed' to check if a certain stock is listed at that given date\n",
    "df_backtest['is_listed_for_one_year'] = (df_backtest.index.get_level_values(level=0) - df_backtest['listed_date'] >= pd.Timedelta('1y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3916346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of non-listed stocks along the time\n",
    "non_listed = df_backtest[~df_backtest['is_listed_for_one_year']]\n",
    "num_nonlisted_stock = non_listed.groupby(level=0).count()['is_listed_for_one_year']\n",
    "num_nonlisted_stock.plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c860009",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468acaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load st/suspend data from Ricequant\n",
    "df_is_st = dl.load_st_data(stock_names)\n",
    "df_is_suspended = dl.load_suspended_data(stock_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4bc424",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2861ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3\n",
    "#create ST and suspended columns\n",
    "df_backtest['is_st'] = df_is_st.values\n",
    "df_backtest['is_suspended'] = df_is_suspended.values\n",
    "#filter out stocks that are listed within a year\n",
    "#filter out ST and suspended stocks, filter data by the stock's listed date\n",
    "df_backtest = df_backtest.loc[ (~df_backtest['is_st']) & (~df_backtest['is_suspended']) & (df_backtest['is_listed_for_one_year']), BASIC_INFO_COLS]\n",
    "#keep data only on the rebalancing dates\n",
    "rebalancing_dates = pd.date_range(start=START_DATE, end=END_DATE, freq='BM')\n",
    "df_backtest = df_backtest[df_backtest.index.get_level_values(0).isin(rebalancing_dates)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65165ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f21aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the current rebalancing date is the last trading day of the current period\n",
    "# 'next_period_open' is defined as the stock's open price on the next relancing date\n",
    "# 'next_period_return' is the generated return by holding a stock from EOD of current rebalancing date to the start of the next rebalancing date\n",
    "df_backtest['next_period_open'] = df_backtest['open'].groupby(level=1).shift(-1).values\n",
    "df_backtest['next_period_return'] = (df_backtest['next_period_open'].values - df_backtest['close'].values) / df_backtest['close'].values\n",
    "df_backtest = df_backtest[df_backtest.index.get_level_values(0) != df_backtest.index.get_level_values(0).max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd78e1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocess1 = df_backtest.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3429bfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backtest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f72b31a",
   "metadata": {},
   "source": [
    "## Data Preprocessing part 2\n",
    "### 1) Replace Outliers with the corresponding threshold\n",
    "### 2) Standardization - Subtract mean and divide by std\n",
    "### 3) Fill missing values with 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44feca15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outlier(df, n=3,):\n",
    "    #for any factor, if the stock's factor exposure lies more than n times MAD away from the factor's median, \n",
    "    # reset that stock's factor exposure to median + n * MAD/median - n * MAD\n",
    "    med = df.median(axis=0)\n",
    "    MAD = (df - med).abs().median()\n",
    "    upper_limit = med + n * MAD\n",
    "    lower_limit = med - n * MAD\n",
    "    print(f\"lower_limit = {lower_limit}, upper_limit = {upper_limit}\")\n",
    "    #pd.DataFrame.where replaces data in the dataframe by 'other' where the condition is False\n",
    "    df = df.where(~( (df > upper_limit) & df.notnull() ) , other = upper_limit, axis=1)\n",
    "    df = df.where(~( (df < lower_limit) & df.notnull() ), other = lower_limit, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cc1490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is this step used for? Check the number of rebalancing dates?\n",
    "df_preprocess1.groupby(level=0)[TEST_FACTORS].agg('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94cd952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1\n",
    "df_preprocess1[TEST_FACTORS] = applyParallel(df_preprocess1[TEST_FACTORS].groupby(level=0), remove_outlier).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c722693",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocess1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30325a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(df):\n",
    "    #on each rebalancing date, each standardized factor has mean 0 and std 1\n",
    "    return (df - df.mean()) / df.std()\n",
    "\n",
    "df_preprocess1[TEST_FACTORS] = applyParallel(df_preprocess1[TEST_FACTORS].groupby(level=0), standardize).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4e4098",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_preprocess1.loc[df_preprocess1.index.get_level_values(0) == '2011-01-31', TEST_FACTORS]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335fae0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = (df - df.mean()) / df.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59bd4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44834127",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95b30b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#after rebalancing, on each rebalancing date, each standardized factor has mean 0 and std 1\n",
    "df_preprocess1.groupby(level=0)[TEST_FACTORS].agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec72fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocess1[TEST_FACTORS] = df_preprocess1[TEST_FACTORS].fillna(0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5e530d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocess1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90ea507",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data missing issue, simply filter them out\n",
    "#this may cause the factor test results in later stages to be incorrectly calculated\n",
    "#the magnitude of error depends on proportion of missing values\n",
    "df_preprocess1 = df_preprocess1[df_preprocess1['next_period_return'].notnull() & df_preprocess1['market_value'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60058091",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backtest = df_preprocess1.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fe55c7",
   "metadata": {},
   "source": [
    "# Single-Factor Backtesting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7a0bdf",
   "metadata": {},
   "source": [
    "同一类风格因子选多个(5-10)集中测试，下面是常见的几种测试方法，对于每一个方法相应的不同评价指标，我们最后用折线图/柱状图和一张完整的表格/dataframe来展示，详见华泰研报单因子测试中的任意一篇\n",
    "\n",
    "- 回归法\n",
    "- IC值\n",
    "- 分层回测（既要与基准组合比较，也要比较超额收益的时间序列）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dbcd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SINGLE_FACTOR = 'PE_TTM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a4ce1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backtest = df_backtest.merge(df_indus_mapping, how='left', left_on='stock', right_index=True)\n",
    "print(df_backtest.shape)\n",
    "df_backtest = df_backtest[df_backtest['pri_indus_code'].notnull()]\n",
    "print(df_backtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732fb9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backtest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b01c3e1",
   "metadata": {},
   "source": [
    "# T-Value Analysis\n",
    "回归法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a5b7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wls_tval_coef(df, SINGLE_FACTOR):\n",
    "    #obtain the t-value in WLS of the tested factor\n",
    "    # 函数内需要用包要额外在这里加上\n",
    "    import statsmodels.formula.api as smf\n",
    "    import pandas as pd\n",
    "\n",
    "    # Weighted Least Square(WLS) uses the square root of market cap of each stock\n",
    "    # 使用加权最小二乘回归，并以个股流通市值的平方根作为权重\n",
    "    # other than the factor of interest, we also regress on the industry for neutralization\n",
    "    # 同时对要测试的因子和行业因子做回归（个股属于该行业为1，否则为0），消除因子收益的行业间差异\n",
    "    wls_result = smf.wls(formula = f\"next_period_return ~ pri_indus_code + {SINGLE_FACTOR}\", \n",
    "                    data=df, weights = df['market_value'] ** 0.5).fit()\n",
    "    result_tval_coef = pd.Series( {'t_value': wls_result.tvalues.values[0], 'coef': wls_result.params.values[0], \n",
    "                         } )\n",
    "    # result_resid = pd.Series( {'resid': wls_result.resid.values} )\n",
    "    return result_tval_coef.to_frame().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b01ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the t-value for all periods\n",
    "from functools import partial\n",
    "SINGLE_FACTOR = 'PE_TTM'\n",
    "wls_results_tval_coef = applyParallel(df_backtest.groupby(level=0), partial(wls_tval_coef, SINGLE_FACTOR=SINGLE_FACTOR))\n",
    "wls_results_tval_coef.index = df_backtest.index.get_level_values(level=0).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b517f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wls_results_tval_coef['t_value'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77feb388",
   "metadata": {},
   "outputs": [],
   "source": [
    "wls_results_tval_coef['coef'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaac3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a summary result from the t-value series\n",
    "# 回归法的因子评价指标\n",
    "\n",
    "# t值序列绝对值平均值\n",
    "tval_series_mean = wls_results_tval_coef['t_value'].abs().mean()\n",
    "# t 值序列绝对值大于 2 的占比\n",
    "large_tval_prop = (wls_results_tval_coef['t_value'].abs() > 2).sum() / wls_results_tval_coef.shape[0]\n",
    "# t 值序列均值的绝对值除以 t 值序列的标准差\n",
    "standardized_tval = wls_results_tval_coef['t_value'].mean() / wls_results_tval_coef['t_value'].std()\n",
    "# 因子收益率序列平均值\n",
    "coef_series_mean = wls_results_tval_coef['coef'].mean()\n",
    "# 因子收益率均值零假设检验的 t 值\n",
    "coef_series_t_val = scipy.stats.ttest_1samp(wls_results_tval_coef['coef'], 0).statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d811c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('t值序列绝对值平均值：', '{:0.4f}'.format(tval_series_mean))\n",
    "print('t值序列绝对值大于2的占比：', '{percent:.2%}'.format(percent = large_tval_prop))\n",
    "print('t 值序列均值的绝对值除以 t 值序列的标准差：', '{:0.4f}'.format(standardized_tval))\n",
    "print('因子收益率均值：', '{percent:.4%}'.format(percent=coef_series_mean))\n",
    "print('因子收益率均值零假设检验的 t 值：', '{:0.4f}'.format(coef_series_t_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2c80c7",
   "metadata": {},
   "source": [
    "## Information Coefficient Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7f7c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rank IC is defined by the spearman correlation of the factor residual(after market-value and industry neutralizations)\n",
    "#with next period's return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4a648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocess of IC analysis\n",
    "# 因子值IC值计算之前的预处理\n",
    "# 因子值在去极值、标准化、去空值处理后，在截面期上用其做因变量对市值因子及行业\n",
    "# 因子（哑变量）做线性回归，取残差作为因子值的一个替代\n",
    "\n",
    "def wls_factor_resid(df):\n",
    "    import statsmodels.formula.api as smf\n",
    "    wls_result = smf.wls(formula = f\"PE_TTM ~ market_value + pri_indus_code\", \n",
    "                    data=df).fit()\n",
    "    return wls_result.resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173f84d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_resids = applyParallel(df_backtest.groupby(level=0), wls_factor_resid)\n",
    "factor_resids = factor_resids.rename('PE_TTM_resid')\n",
    "factor_resids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b16c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backtest = df_backtest.merge(factor_resids, how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d20760",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e5576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下一期所有个股的收益率向量和当期因子的暴露度向量的相关系数\n",
    "# use Spearman's rank correlation coefficient by default. Another choice is Pearson\n",
    "def cross_sectional_ic(df):\n",
    "    return df[['next_period_return', 'PE_TTM_resid']].corr(method='spearman').iloc[0, 1]\n",
    "ic_series = df_backtest.groupby(level=0).apply(cross_sectional_ic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a90ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e70bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_series_mean = ic_series.mean()\n",
    "ic_series_std = ic_series.std()\n",
    "ir = ic_series_mean / ic_series_std\n",
    "ic_series_cum = ic_series.cumsum()\n",
    "ic_pos_prop = (ic_series > 0).sum() / ic_series.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e737b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('IC 均值:','{:0.4f}'.format(ic_series_mean))\n",
    "print('IC 标准差:','{:0.4f}'.format(ic_series_std))\n",
    "print('IR 比率:','{percent:.2%}'.format(percent=ir))\n",
    "print('IC 值序列大于零的占比:','{percent:.2%}'.format(percent=ic_pos_prop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6942c477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IC 值累积曲线——随时间变化效果是否稳定\n",
    "ic_series_cum.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db866a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backtest = df_backtest.drop(columns = ['PE_TTM_resid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecaa1bd",
   "metadata": {},
   "source": [
    "## Hiearachical Backtesting\n",
    "分层回测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd9ed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_GROUPS = 5\n",
    "GROUP_NAMES = [f\"group{i}_weight\" for i in range(1, NUM_GROUPS + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55bfb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "assigned_group = df_backtest.groupby('pri_indus_code')['PE_TTM'].apply(partial(pd.qcut, q=5, labels=range(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b03a5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e5b130",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10, 5))\n",
    "num_stocks_per_indus = df_backtest[df_backtest.index.get_level_values(0) == '2011-01-31'].groupby('pri_indus_code')['PE_TTM'].count()\n",
    "\n",
    "plt.bar(*zip(*num_stocks_per_indus.items()))\n",
    " \n",
    "plt.xlabel(\"Industry\")\n",
    "plt.ylabel(\"Number of Companies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ddcc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backtest = df_backtest.merge(df_index, how='left', left_on='date', right_index=True, )\n",
    "df_backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2140a88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here for simplicity we assume that index weight is a uniform portfolio over all stocks, to be modified later\n",
    "df_backtest['index_weight'] = df_backtest.groupby(level=0).apply(lambda df: pd.Series([1/df.shape[0]] * df.shape[0])).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498cffe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the industry weights of the index onto the backtesting dataframe\n",
    "def set_index_indus_weight(df):\n",
    "    index_indus_weight = df.groupby('pri_indus_code')['index_weight'].sum().rename('index_indus_weight')\n",
    "    df = df.merge(index_indus_weight, how='left', left_on='pri_indus_code', right_index=True)\n",
    "    return df\n",
    "#'index_indus_weight' = this stock's industry weight in the benchmark index\n",
    "if 'index_indus_weight' not in df_backtest.columns:\n",
    "    df_backtest = applyParallel(df_backtest.groupby(level=0), set_index_indus_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a50a011",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874f5b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# hiearchical backtesting is pretty hard to implement using pure vectorization/parrallelization, and I have to use for loop at least once.\n",
    "def get_group_weight_by_industry(num_stocks, num_groups) -> np.array:\n",
    "    \"\"\"\n",
    "    precondition: the stocks need to be sorted by factor exposure\n",
    "    @num_stocks: the number of stocks in this industry\n",
    "    @num_groups: the number of portfolio groups to be constructed\n",
    "    \n",
    "    returns: an intermediary (num_stocks x num_groups) weight matrix specifying the weight of \n",
    "             each stock in each group. This is not the final weight matrix because there are many industries, so that the final weights within \n",
    "             each group should be smaller than 1. Here the returned\n",
    "             weight matrix represents the weight distribution within a single industry, so the weights adds up to 1.\n",
    "    \n",
    "    if you want to understand the algorithm deeper, print some intermediary outputs\n",
    "    \"\"\"\n",
    "    num_rows = min(num_groups, num_stocks)\n",
    "    num_cols = max(num_groups, num_stocks)\n",
    "    weight_mat = np.zeros((num_rows, num_cols))\n",
    "    remaining = 0\n",
    "    j = 0\n",
    "    row_budget = num_cols\n",
    "    col_budget = num_rows\n",
    "    for i in range(num_rows):\n",
    "        # print(f\"i = {i}\")\n",
    "        start = col_budget - remaining\n",
    "        # print(f\"start = {start}\")\n",
    "        weight_mat[i, j] = start\n",
    "        offset = (row_budget - start) // col_budget\n",
    "        # print(f\"offset = {offset}\")\n",
    "        weight_mat[i, j + 1: j + 1 + offset] = col_budget\n",
    "        remaining = row_budget - offset * col_budget - start\n",
    "        j = j + 1 + offset\n",
    "        if j < num_cols:\n",
    "            weight_mat[i, j] = remaining\n",
    "        \n",
    "    weight_mat = weight_mat if num_groups > num_stocks else weight_mat.transpose()\n",
    "    weight_mat_normalized = weight_mat / weight_mat.sum(axis=0)\n",
    "    return weight_mat_normalized\n",
    "\n",
    "def get_weight_df_by_industry(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"get the weight dataframe for each industry\"\"\"\n",
    "    #sort by the factor exposure\n",
    "    df = df.sort_values(by='PE_TTM') \n",
    "    stock_names = df.index.get_level_values(1)\n",
    "    #get weight matrix first\n",
    "    weight_mat = get_group_weight_by_industry(stock_names.shape[0], NUM_GROUPS)\n",
    "    df[GROUP_NAMES] = weight_mat\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdd42bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_weight_by_date(df_backtest_sub):\n",
    "    #get the intermediary weights in each group on each rebalancing date\n",
    "    df_backtest_sub = df_backtest_sub.groupby('pri_indus_code').apply(get_weight_df_by_industry).droplevel(0).sort_index(level=1)\n",
    "    \"\"\"\n",
    "    we need to make the group portfolio industry-neutral with the index. That is, industry weights should be the same in both\n",
    "    the group portfolio and the index. \n",
    "    \"\"\"\n",
    "    #multiply each stock's intermediary weight by its industry weight. since the intermediary weight within each group within each industry adds up to 1(as explained in the previous function),\n",
    "    #after this operation the final stock weight within each group should add up to 1.\n",
    "    df_backtest_sub[GROUP_NAMES] = np.multiply(df_backtest_sub[GROUP_NAMES].values, df_backtest_sub['index_indus_weight'].values[:, np.newaxis])\n",
    "    return df_backtest_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd22899",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backtest = df_backtest.groupby(level=0).apply(get_group_weight_by_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3274685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9044ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backtest.groupby(level=0)[GROUP_NAMES].sum() #looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3008e0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_returns_by_date(df_backtest_sub):\n",
    "    group_returns = df_backtest_sub[GROUP_NAMES].values.transpose() @ df_backtest_sub['next_period_return'].values\n",
    "    group_returns = pd.Series(group_returns, index=GROUP_NAMES)\n",
    "    return group_returns\n",
    "\n",
    "group_returns_by_date = df_backtest.groupby(level=0).apply(get_group_returns_by_date)\n",
    "group_returns_by_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00abe138",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_cum_returns= (group_returns_by_date + 1).cumprod(axis=0)\n",
    "group_cum_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68551a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_cum_returns.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dfa1e6",
   "metadata": {},
   "source": [
    "## Factor Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13eff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMBINE_FACTORS = ['PE_TTM', 'PS_TTM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fb970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ic_series(factor, df_backtest=df_backtest):\n",
    "    def wls_factor_resid(df):\n",
    "        import statsmodels.formula.api as smf\n",
    "        wls_result = smf.wls(formula = f\"{factor} ~ 0 + market_value + C(pri_indus_code)\", \n",
    "                        data=df, weights = df['market_value'] ** 0.5).fit()\n",
    "        return wls_result.resid\n",
    "    if f'{factor}_resid' not in df_backtest.columns:\n",
    "        factor_resids = applyParallel(df_backtest.groupby(level=0), wls_factor_resid)\n",
    "        factor_resids = factor_resids.rename(f'{factor}_resid')\n",
    "        df_backtest = df_backtest.merge(factor_resids, how='left', left_index=True, right_index=True)\n",
    "    def cross_sectional_ic(df):\n",
    "        return df[['next_period_return', f'{factor}_resid']].corr(method='spearman').iloc[0, 1]\n",
    "    ic_series = df_backtest.groupby(level=0).apply(cross_sectional_ic)\n",
    "    return ic_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28cd996",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4efdb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The multiprocessing takes forever, not sure why\n",
    "#had to use for loop for now. Look into this later\n",
    "\n",
    "# with pathos.multiprocessing.ProcessPool(pathos.helpers.cpu_count()) as pool: \n",
    "#     ic_series_results = pool.map( get_ic_series, COMBINE_FACTORS)\n",
    "ic_series_results = [get_ic_series(factor).rename(factor) for factor in COMBINE_FACTORS] #around 8 seconds\n",
    "df_ic_series = pd.concat(ic_series_results, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b37effb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ic_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7860daea",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_periods = 12\n",
    "df_ic_series.rolling(hist_periods, min_periods=hist_periods).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a723f361",
   "metadata": {},
   "outputs": [],
   "source": [
    "#leave the computation for later\n",
    "# A = np.arange(6).reshape(2,3) # 2 x 3, N = 2, T = 3\n",
    "# S = pd.DataFrame(A).T.cov() #2 x 2\n",
    "# np.expand_dims(A, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b1bd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ic_cov_mat_series = df_ic_series.rolling(hist_periods, min_periods=hist_periods).cov()\n",
    "df_ic_cov_mat_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09847baf",
   "metadata": {},
   "source": [
    "#### maximize the ICIR values on a single rebalancing date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5664de0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af708c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.date_range(START_DATE, END_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2fb228",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ic_hist_mean = df_ic_series.rolling(hist_periods, min_periods=hist_periods).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76df536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ic_cov_mat_series = df_ic_series.rolling(hist_periods, min_periods=hist_periods).cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26568d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ic = df_ic_series[df_ic_series.index == '2020-09-30']\n",
    "df_ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa0d11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ic_cov_mat = df_ic_cov_mat_series[df_ic_cov_mat_series.index.get_level_values(0) == '2020-09-30']\n",
    "df_ic_cov_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7463346a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w @ df_ic_cov_mat.values @ w.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f406179",
   "metadata": {},
   "outputs": [],
   "source": [
    "w.transpose() @ df_ic.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3ea7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ic_ir(factor_weights):\n",
    "    ic_mean = factor_weights.transpose() @ df_ic.values.flatten()\n",
    "    ic_var = factor_weights @ df_ic_cov_mat.values @ factor_weights.transpose()\n",
    "    return ic_mean / (ic_var ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5ede5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ic_cov_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec77981",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_factors = len(COMBINE_FACTORS)\n",
    "opt_result = scipy.optimize.minimize(\n",
    "                lambda w: -get_ic_ir(w),\n",
    "                np.array([1 / num_factors] * num_factors),\n",
    "                bounds=[(0, 1) for i in range(num_factors)],\n",
    "                constraints=({\"type\": \"eq\", \"fun\": lambda weight: np.sum(weight) - 1})\n",
    "            )\n",
    "opt_factor_weight = opt_result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92cd5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_factor_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c60e6f",
   "metadata": {},
   "source": [
    "#### optimal factor weight on all rebalancing dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a618c7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ic_series.rolling(12).apply(lambda df: df.mean() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff2d699",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ic_series.index[hist_periods:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14046b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_opt_factor_weight_by_date(date): \n",
    "    df_ic = df_ic_series[df_ic_series.index == date]\n",
    "    df_pred_ic = df_ic.rolling(hist_periods).mean()\n",
    "    print('11111')\n",
    "    print(df_pred_ic)\n",
    "    df_pred_ic_cov_mat = df_ic.rolling(hist_periods).cov()\n",
    "    print('a')\n",
    "    def get_ic_ir(factor_weights):\n",
    "        combined_ic_mean = factor_weights.transpose() @ df_pred_ic.values.flatten()\n",
    "        combined_ic_var = factor_weights @ df_pred_ic_cov_mat.values @ factor_weights.transpose()\n",
    "        combined_ir = combined_ic_mean / (combined_ic_var ** 0.5)\n",
    "        return combined_ir\n",
    "    print('b')\n",
    "    opt_result = scipy.optimize.minimize(\n",
    "                lambda w: -get_ic_ir(w),\n",
    "                np.array([1 / num_factors] * num_factors),\n",
    "                bounds=[(0, 1) for i in range(num_factors)],\n",
    "                constraints=({\"type\": \"eq\", \"fun\": lambda weight: np.sum(weight) - 1})\n",
    "            )\n",
    "    print('c')\n",
    "    opt_factor_weight = pd.Series(opt_result.x, index=COMBINE_FACTORS, name=date)\n",
    "    print('d')\n",
    "    return opt_result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b62efd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dates = df_ic_series.index[hist_periods:]\n",
    "results = [get_opt_factor_weight_by_date(date) for date in valid_dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f617ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "results #TBD: not sure why the optimization is not working, debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeabf758",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we cannot use pandas.rolling.apply(func) because rolling.apply is different from groupby.apply -- it cannot take a dataframe as the parameter\n",
    "\n",
    "\n",
    "# num_factors = len(COMBINE_FACTORS)\n",
    "# def get_opt_factor_weight_by_date(ic_series: pd.Series): \n",
    "#     df = df_ic_series.loc[ic_series.index, :]\n",
    "#     df_pred_ic = df.mean()\n",
    "#     print('11111')\n",
    "#     print(df_pred_ic)\n",
    "#     df_pred_ic_cov_mat = df.cov()\n",
    "#     print('a')\n",
    "#     def get_ic_ir(factor_weights):\n",
    "#         combined_ic_mean = factor_weights.transpose() @ df_pred_ic.values.flatten()\n",
    "#         combined_ic_var = factor_weights @ df_pred_ic_cov_mat.values @ factor_weights.transpose()\n",
    "#         combined_ir = combined_ic_mean / (combined_ic_var ** 0.5)\n",
    "#         return combined_ir\n",
    "#     print('b')\n",
    "#     opt_result = scipy.optimize.minimize(\n",
    "#                 lambda w: -get_ic_ir(w),\n",
    "#                 np.array([1 / num_factors] * num_factors),\n",
    "#                 bounds=[(0, 1) for i in range(num_factors)],\n",
    "#                 constraints=({\"type\": \"eq\", \"fun\": lambda weight: np.sum(weight) - 1})\n",
    "#             )\n",
    "#     print('c')\n",
    "#     date = df.index[0]\n",
    "#     opt_factor_weight = pd.Series(opt_result.x, index=COMBINE_FACTORS, name=date)\n",
    "#     print('d')\n",
    "#     return opt_result.x\n",
    "\n",
    "# df_ic_series.rolling(12).apply(get_opt_factor_weight_by_date)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('multifactor': conda)",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
